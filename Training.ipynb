{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a87a0c9f-47a1-4229-8ad1-eef24ce0ea1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU 48\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /home/daoqm/data-cifar10/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:02<00:00, 75.0MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/daoqm/data-cifar10/cifar-10-python.tar.gz to /home/daoqm/data-cifar10\n",
      "Files already downloaded and verified\n",
      "[01/300] train acc 32.011% | val acc 36.571%\n",
      "[02/300] train acc 43.188% | val acc 46.560%\n",
      "[03/300] train acc 49.383% | val acc 50.976%\n",
      "[04/300] train acc 53.232% | val acc 54.546%\n",
      "[05/300] train acc 56.962% | val acc 55.934%\n",
      "[06/300] train acc 59.821% | val acc 56.848%\n",
      "[07/300] train acc 61.496% | val acc 59.769%\n",
      "[08/300] train acc 63.760% | val acc 61.370%\n",
      "[09/300] train acc 65.190% | val acc 61.143%\n",
      "[10/300] train acc 66.365% | val acc 65.310%\n",
      "[11/300] train acc 68.148% | val acc 64.653%\n",
      "[12/300] train acc 69.574% | val acc 66.719%\n",
      "[13/300] train acc 71.046% | val acc 67.757%\n",
      "[14/300] train acc 72.109% | val acc 68.529%\n",
      "[15/300] train acc 73.608% | val acc 67.686%\n",
      "[16/300] train acc 74.655% | val acc 69.532%\n",
      "[17/300] train acc 75.524% | val acc 71.528%\n",
      "[18/300] train acc 76.744% | val acc 71.327%\n",
      "[19/300] train acc 77.903% | val acc 71.090%\n",
      "[20/300] train acc 78.971% | val acc 71.759%\n",
      "[21/300] train acc 79.646% | val acc 71.808%\n",
      "[22/300] train acc 80.753% | val acc 72.404%\n",
      "[23/300] train acc 81.894% | val acc 73.087%\n",
      "[24/300] train acc 82.897% | val acc 71.795%\n",
      "[25/300] train acc 83.731% | val acc 74.551%\n",
      "[26/300] train acc 84.667% | val acc 73.800%\n",
      "[27/300] train acc 85.654% | val acc 73.338%\n",
      "[28/300] train acc 86.818% | val acc 73.795%\n",
      "[29/300] train acc 87.347% | val acc 74.540%\n",
      "[30/300] train acc 88.238% | val acc 74.045%\n",
      "[31/300] train acc 89.242% | val acc 73.678%\n",
      "[32/300] train acc 90.310% | val acc 73.035%\n",
      "[33/300] train acc 90.761% | val acc 73.594%\n",
      "[34/300] train acc 91.634% | val acc 74.435%\n",
      "[35/300] train acc 92.123% | val acc 73.358%\n",
      "[36/300] train acc 92.696% | val acc 74.197%\n",
      "[37/300] train acc 93.299% | val acc 74.006%\n",
      "[38/300] train acc 93.369% | val acc 73.555%\n",
      "[39/300] train acc 94.409% | val acc 73.716%\n",
      "[40/300] train acc 94.356% | val acc 73.991%\n",
      "[41/300] train acc 94.922% | val acc 73.534%\n",
      "[42/300] train acc 95.026% | val acc 73.691%\n",
      "[43/300] train acc 95.927% | val acc 74.357%\n",
      "[44/300] train acc 95.453% | val acc 74.241%\n",
      "[45/300] train acc 95.755% | val acc 73.254%\n",
      "[46/300] train acc 96.350% | val acc 73.279%\n",
      "[47/300] train acc 96.523% | val acc 73.869%\n",
      "[48/300] train acc 96.290% | val acc 74.143%\n",
      "[49/300] train acc 96.535% | val acc 73.345%\n",
      "[50/300] train acc 96.987% | val acc 73.283%\n",
      "[51/300] train acc 96.783% | val acc 73.992%\n",
      "[52/300] train acc 96.991% | val acc 73.728%\n",
      "[53/300] train acc 96.846% | val acc 74.363%\n",
      "[54/300] train acc 97.385% | val acc 74.221%\n",
      "[55/300] train acc 97.214% | val acc 73.735%\n",
      "[56/300] train acc 97.099% | val acc 74.805%\n",
      "[57/300] train acc 97.430% | val acc 74.643%\n",
      "[58/300] train acc 97.682% | val acc 74.107%\n",
      "[59/300] train acc 97.556% | val acc 75.042%\n",
      "[60/300] train acc 97.326% | val acc 74.161%\n",
      "[61/300] train acc 97.816% | val acc 73.977%\n",
      "[62/300] train acc 97.799% | val acc 73.651%\n",
      "[63/300] train acc 97.863% | val acc 72.312%\n",
      "[64/300] train acc 97.587% | val acc 75.228%\n",
      "[65/300] train acc 98.054% | val acc 74.325%\n",
      "[66/300] train acc 97.916% | val acc 74.955%\n",
      "[67/300] train acc 98.094% | val acc 74.731%\n",
      "[68/300] train acc 98.216% | val acc 74.532%\n",
      "[69/300] train acc 97.719% | val acc 74.962%\n",
      "[70/300] train acc 97.767% | val acc 73.896%\n",
      "[71/300] train acc 98.309% | val acc 73.460%\n",
      "[72/300] train acc 98.074% | val acc 73.431%\n",
      "[73/300] train acc 98.558% | val acc 74.591%\n",
      "[74/300] train acc 97.981% | val acc 73.971%\n",
      "[75/300] train acc 98.267% | val acc 75.156%\n",
      "[76/300] train acc 98.314% | val acc 75.131%\n",
      "[77/300] train acc 98.314% | val acc 72.740%\n",
      "[78/300] train acc 98.265% | val acc 74.801%\n",
      "[79/300] train acc 98.705% | val acc 74.180%\n",
      "[80/300] train acc 98.127% | val acc 73.907%\n",
      "[81/300] train acc 98.567% | val acc 73.434%\n",
      "[82/300] train acc 98.358% | val acc 74.802%\n",
      "[83/300] train acc 98.654% | val acc 74.109%\n",
      "[84/300] train acc 98.334% | val acc 74.601%\n",
      "[85/300] train acc 98.340% | val acc 75.590%\n",
      "[86/300] train acc 98.829% | val acc 74.595%\n",
      "[87/300] train acc 98.644% | val acc 73.824%\n",
      "[88/300] train acc 98.453% | val acc 74.834%\n",
      "[89/300] train acc 98.880% | val acc 74.603%\n",
      "[90/300] train acc 98.403% | val acc 74.165%\n",
      "[91/300] train acc 98.674% | val acc 74.843%\n",
      "[92/300] train acc 98.438% | val acc 75.665%\n",
      "[93/300] train acc 99.036% | val acc 75.770%\n",
      "[94/300] train acc 98.603% | val acc 72.978%\n",
      "[95/300] train acc 98.587% | val acc 75.166%\n",
      "[96/300] train acc 99.436% | val acc 74.888%\n",
      "[97/300] train acc 98.449% | val acc 74.623%\n",
      "[98/300] train acc 98.729% | val acc 74.506%\n",
      "[99/300] train acc 98.760% | val acc 75.724%\n",
      "[100/300] train acc 98.545% | val acc 74.017%\n",
      "[101/300] train acc 98.942% | val acc 75.882%\n",
      "[102/300] train acc 99.302% | val acc 75.096%\n",
      "[103/300] train acc 98.633% | val acc 74.792%\n",
      "[104/300] train acc 99.087% | val acc 75.044%\n",
      "[105/300] train acc 99.062% | val acc 75.476%\n",
      "[106/300] train acc 98.498% | val acc 74.455%\n",
      "[107/300] train acc 98.912% | val acc 75.558%\n",
      "[108/300] train acc 99.053% | val acc 74.570%\n",
      "[109/300] train acc 98.972% | val acc 73.986%\n",
      "[110/300] train acc 98.678% | val acc 74.982%\n",
      "[111/300] train acc 99.125% | val acc 74.666%\n",
      "[112/300] train acc 99.251% | val acc 76.162%\n",
      "[113/300] train acc 98.860% | val acc 73.389%\n",
      "[114/300] train acc 98.798% | val acc 75.191%\n",
      "[115/300] train acc 99.220% | val acc 74.893%\n",
      "[116/300] train acc 98.829% | val acc 74.727%\n",
      "[117/300] train acc 99.114% | val acc 74.594%\n",
      "[118/300] train acc 98.940% | val acc 75.554%\n",
      "[119/300] train acc 99.373% | val acc 75.260%\n",
      "[120/300] train acc 99.136% | val acc 74.591%\n",
      "[121/300] train acc 98.849% | val acc 75.598%\n",
      "[122/300] train acc 99.107% | val acc 75.082%\n",
      "[123/300] train acc 99.407% | val acc 75.398%\n",
      "[124/300] train acc 98.843% | val acc 74.366%\n",
      "[125/300] train acc 98.789% | val acc 75.347%\n",
      "[126/300] train acc 99.440% | val acc 75.447%\n",
      "[127/300] train acc 99.445% | val acc 74.734%\n",
      "[128/300] train acc 98.516% | val acc 74.726%\n",
      "[129/300] train acc 99.273% | val acc 75.091%\n",
      "[130/300] train acc 99.543% | val acc 75.624%\n",
      "[131/300] train acc 99.098% | val acc 75.307%\n",
      "[132/300] train acc 98.849% | val acc 76.655%\n",
      "[133/300] train acc 99.480% | val acc 75.047%\n",
      "[134/300] train acc 99.309% | val acc 74.644%\n",
      "[135/300] train acc 98.998% | val acc 74.666%\n",
      "[136/300] train acc 99.040% | val acc 75.275%\n",
      "[137/300] train acc 99.318% | val acc 75.175%\n",
      "[138/300] train acc 99.283% | val acc 74.848%\n",
      "[139/300] train acc 99.067% | val acc 75.131%\n",
      "[140/300] train acc 99.238% | val acc 75.742%\n",
      "[141/300] train acc 98.876% | val acc 75.217%\n",
      "[142/300] train acc 99.618% | val acc 76.597%\n",
      "[143/300] train acc 99.522% | val acc 75.101%\n",
      "[144/300] train acc 98.909% | val acc 73.265%\n",
      "[145/300] train acc 98.787% | val acc 75.880%\n",
      "[146/300] train acc 99.582% | val acc 76.405%\n",
      "[147/300] train acc 99.527% | val acc 76.050%\n",
      "[148/300] train acc 98.798% | val acc 75.338%\n",
      "[149/300] train acc 99.387% | val acc 74.987%\n",
      "[150/300] train acc 99.534% | val acc 75.628%\n",
      "[151/300] train acc 99.082% | val acc 74.888%\n",
      "[152/300] train acc 99.182% | val acc 75.149%\n",
      "[153/300] train acc 99.607% | val acc 76.443%\n",
      "[154/300] train acc 99.520% | val acc 74.646%\n",
      "[155/300] train acc 98.985% | val acc 75.792%\n",
      "[156/300] train acc 99.458% | val acc 75.140%\n",
      "[157/300] train acc 99.320% | val acc 75.651%\n",
      "[158/300] train acc 99.194% | val acc 75.707%\n",
      "[159/300] train acc 99.298% | val acc 73.692%\n",
      "[160/300] train acc 99.469% | val acc 75.986%\n",
      "[161/300] train acc 99.705% | val acc 74.632%\n",
      "[162/300] train acc 98.974% | val acc 75.093%\n",
      "[163/300] train acc 99.456% | val acc 73.855%\n",
      "[164/300] train acc 99.398% | val acc 75.206%\n",
      "[165/300] train acc 99.360% | val acc 74.618%\n",
      "[166/300] train acc 98.967% | val acc 74.097%\n",
      "[167/300] train acc 99.467% | val acc 76.250%\n",
      "[168/300] train acc 99.689% | val acc 75.981%\n",
      "[169/300] train acc 99.254% | val acc 75.594%\n",
      "[170/300] train acc 99.300% | val acc 74.560%\n",
      "[171/300] train acc 99.224% | val acc 75.407%\n",
      "[172/300] train acc 99.582% | val acc 75.344%\n",
      "[173/300] train acc 99.356% | val acc 74.971%\n",
      "[174/300] train acc 99.258% | val acc 75.322%\n",
      "[175/300] train acc 99.416% | val acc 75.414%\n",
      "[176/300] train acc 99.749% | val acc 75.367%\n",
      "[177/300] train acc 99.493% | val acc 74.411%\n",
      "[178/300] train acc 99.009% | val acc 74.158%\n",
      "[179/300] train acc 99.529% | val acc 74.957%\n",
      "[180/300] train acc 99.418% | val acc 75.821%\n",
      "[181/300] train acc 99.534% | val acc 74.858%\n",
      "[182/300] train acc 99.402% | val acc 74.866%\n",
      "[183/300] train acc 99.242% | val acc 75.430%\n",
      "[184/300] train acc 99.391% | val acc 74.731%\n",
      "[185/300] train acc 99.542% | val acc 74.741%\n",
      "[186/300] train acc 99.707% | val acc 75.646%\n",
      "[187/300] train acc 99.418% | val acc 74.118%\n",
      "[188/300] train acc 99.016% | val acc 74.973%\n",
      "[189/300] train acc 99.593% | val acc 75.960%\n",
      "[190/300] train acc 99.893% | val acc 76.164%\n",
      "[191/300] train acc 99.193% | val acc 73.910%\n",
      "[192/300] train acc 99.280% | val acc 75.232%\n",
      "[193/300] train acc 99.602% | val acc 74.606%\n",
      "[194/300] train acc 99.489% | val acc 75.289%\n",
      "[195/300] train acc 99.487% | val acc 74.903%\n",
      "[196/300] train acc 99.433% | val acc 75.643%\n",
      "[197/300] train acc 99.296% | val acc 75.943%\n",
      "[198/300] train acc 99.729% | val acc 75.416%\n",
      "[199/300] train acc 99.822% | val acc 76.059%\n",
      "[200/300] train acc 99.169% | val acc 75.438%\n",
      "[201/300] train acc 99.351% | val acc 75.447%\n",
      "[202/300] train acc 99.460% | val acc 75.064%\n",
      "[203/300] train acc 99.898% | val acc 75.396%\n",
      "[204/300] train acc 99.709% | val acc 76.180%\n",
      "[205/300] train acc 98.969% | val acc 75.646%\n",
      "[206/300] train acc 99.569% | val acc 75.897%\n",
      "[207/300] train acc 99.778% | val acc 76.534%\n",
      "[208/300] train acc 99.782% | val acc 75.563%\n",
      "[209/300] train acc 99.180% | val acc 74.643%\n",
      "[210/300] train acc 99.293% | val acc 75.762%\n",
      "[211/300] train acc 99.751% | val acc 75.989%\n",
      "[212/300] train acc 99.609% | val acc 74.636%\n",
      "[213/300] train acc 99.594% | val acc 74.213%\n",
      "[214/300] train acc 99.180% | val acc 74.670%\n",
      "[215/300] train acc 99.485% | val acc 75.295%\n",
      "[216/300] train acc 99.940% | val acc 76.875%\n",
      "[217/300] train acc 100.000% | val acc 76.679%\n",
      "[218/300] train acc 100.000% | val acc 76.667%\n",
      "[219/300] train acc 100.000% | val acc 76.929%\n",
      "[220/300] train acc 100.000% | val acc 76.778%\n",
      "[221/300] train acc 100.000% | val acc 77.273%\n",
      "[222/300] train acc 100.000% | val acc 76.849%\n",
      "[223/300] train acc 100.000% | val acc 76.881%\n",
      "[224/300] train acc 100.000% | val acc 76.733%\n",
      "[225/300] train acc 100.000% | val acc 76.688%\n",
      "[226/300] train acc 100.000% | val acc 76.978%\n",
      "[227/300] train acc 100.000% | val acc 77.000%\n",
      "[228/300] train acc 100.000% | val acc 76.952%\n",
      "[229/300] train acc 100.000% | val acc 76.991%\n",
      "[230/300] train acc 100.000% | val acc 77.104%\n",
      "[231/300] train acc 100.000% | val acc 77.188%\n",
      "[232/300] train acc 100.000% | val acc 76.846%\n",
      "[233/300] train acc 100.000% | val acc 77.272%\n",
      "[234/300] train acc 100.000% | val acc 77.201%\n",
      "[235/300] train acc 100.000% | val acc 76.947%\n",
      "[236/300] train acc 100.000% | val acc 77.102%\n",
      "[237/300] train acc 100.000% | val acc 77.005%\n",
      "[238/300] train acc 100.000% | val acc 77.102%\n",
      "[239/300] train acc 100.000% | val acc 77.142%\n",
      "[240/300] train acc 100.000% | val acc 77.171%\n",
      "[241/300] train acc 100.000% | val acc 77.495%\n",
      "[242/300] train acc 100.000% | val acc 77.220%\n",
      "[243/300] train acc 100.000% | val acc 77.399%\n",
      "[244/300] train acc 100.000% | val acc 77.134%\n",
      "[245/300] train acc 100.000% | val acc 77.081%\n",
      "[246/300] train acc 100.000% | val acc 77.209%\n",
      "[247/300] train acc 100.000% | val acc 77.213%\n",
      "[248/300] train acc 100.000% | val acc 76.949%\n",
      "[249/300] train acc 100.000% | val acc 77.164%\n",
      "[250/300] train acc 100.000% | val acc 77.271%\n",
      "[251/300] train acc 100.000% | val acc 77.257%\n",
      "[252/300] train acc 100.000% | val acc 77.275%\n",
      "[253/300] train acc 100.000% | val acc 77.331%\n",
      "[254/300] train acc 100.000% | val acc 77.196%\n",
      "[255/300] train acc 100.000% | val acc 77.051%\n",
      "[256/300] train acc 100.000% | val acc 77.218%\n",
      "[257/300] train acc 100.000% | val acc 77.085%\n",
      "[258/300] train acc 100.000% | val acc 77.148%\n",
      "[259/300] train acc 100.000% | val acc 77.054%\n",
      "[260/300] train acc 100.000% | val acc 77.306%\n",
      "[261/300] train acc 100.000% | val acc 77.454%\n",
      "[262/300] train acc 100.000% | val acc 77.244%\n",
      "[263/300] train acc 100.000% | val acc 77.366%\n",
      "[264/300] train acc 100.000% | val acc 77.156%\n",
      "[265/300] train acc 100.000% | val acc 77.103%\n",
      "[266/300] train acc 100.000% | val acc 77.494%\n",
      "[267/300] train acc 100.000% | val acc 77.132%\n",
      "[268/300] train acc 100.000% | val acc 77.115%\n",
      "[269/300] train acc 100.000% | val acc 77.281%\n",
      "[270/300] train acc 100.000% | val acc 77.110%\n",
      "[271/300] train acc 100.000% | val acc 77.151%\n",
      "[272/300] train acc 100.000% | val acc 77.248%\n",
      "[273/300] train acc 100.000% | val acc 77.369%\n",
      "[274/300] train acc 100.000% | val acc 77.136%\n",
      "[275/300] train acc 100.000% | val acc 77.066%\n",
      "[276/300] train acc 100.000% | val acc 77.170%\n",
      "[277/300] train acc 100.000% | val acc 77.133%\n",
      "[278/300] train acc 100.000% | val acc 77.307%\n",
      "[279/300] train acc 100.000% | val acc 77.347%\n",
      "[280/300] train acc 100.000% | val acc 77.446%\n",
      "[281/300] train acc 100.000% | val acc 77.509%\n",
      "[282/300] train acc 100.000% | val acc 77.455%\n",
      "[283/300] train acc 100.000% | val acc 77.648%\n",
      "[284/300] train acc 100.000% | val acc 77.352%\n",
      "[285/300] train acc 100.000% | val acc 77.590%\n",
      "[286/300] train acc 100.000% | val acc 77.474%\n",
      "[287/300] train acc 100.000% | val acc 77.591%\n",
      "[288/300] train acc 100.000% | val acc 77.467%\n",
      "[289/300] train acc 100.000% | val acc 77.570%\n",
      "[290/300] train acc 100.000% | val acc 77.487%\n",
      "[291/300] train acc 100.000% | val acc 77.487%\n",
      "[292/300] train acc 100.000% | val acc 77.385%\n",
      "[293/300] train acc 100.000% | val acc 77.302%\n",
      "[294/300] train acc 100.000% | val acc 77.493%\n",
      "[295/300] train acc 100.000% | val acc 77.337%\n",
      "[296/300] train acc 100.000% | val acc 77.381%\n",
      "[297/300] train acc 100.000% | val acc 77.587%\n",
      "[298/300] train acc 100.000% | val acc 77.628%\n",
      "[299/300] train acc 100.000% | val acc 77.430%\n",
      "[300/300] train acc 100.000% | val acc 77.459%\n"
     ]
    }
   ],
   "source": [
    "# import torchvision.transforms as T\n",
    "# import torchvision.datasets as ds\n",
    "# from torch.utils.data import DataLoader, random_split\n",
    "# import os\n",
    "# import pathlib\n",
    "# import torch, torch.nn as nn, torch.optim as optim\n",
    "# import timm\n",
    "# from torchmetrics.classification import MulticlassAccuracy\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "# torch.cuda.set_device(4)\n",
    "\n",
    "# BATCH      = 128           \n",
    "# IMG_SIZE   = 224             \n",
    "# WORKERS    = os.cpu_count() \n",
    "# print('Number of CPU', WORKERS)\n",
    "\n",
    "# train_tfms = T.Compose([\n",
    "#     T.Resize(IMG_SIZE),\n",
    "#     T.RandomHorizontalFlip(),\n",
    "#     T.ToTensor(),\n",
    "#     T.Normalize(mean=(0.5,)*3, std=(0.5,)*3),\n",
    "# ])\n",
    "\n",
    "# test_tfms  = T.Compose([\n",
    "#     T.Resize(IMG_SIZE),\n",
    "#     T.ToTensor(),\n",
    "#     T.Normalize(mean=(0.5,)*3, std=(0.5,)*3),\n",
    "# ])\n",
    "\n",
    "# root = pathlib.Path.home() / \"data-cifar10\"\n",
    "# full_train = ds.CIFAR10(root, train=True,  download=True, transform=train_tfms)\n",
    "# test_set   = ds.CIFAR10(root, train=False, download=True, transform=test_tfms)\n",
    "\n",
    "# train_set, val_set = random_split(full_train, [45_000, 5_000],\n",
    "#                                   generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# train_loader = DataLoader(train_set, BATCH, shuffle=True,  num_workers=WORKERS)\n",
    "# val_loader   = DataLoader(val_set,   BATCH, shuffle=False, num_workers=WORKERS)\n",
    "# test_loader  = DataLoader(test_set,  BATCH, shuffle=False, num_workers=WORKERS)\n",
    "\n",
    "\n",
    "# DEVICE = \"cuda\" \n",
    "# NUM_EPOCHS  = 300            \n",
    "# LR          = 3e-4\n",
    "# MODEL_NAME  = \"deit_tiny_patch16_224\"\n",
    "\n",
    "# model = timm.create_model(MODEL_NAME,\n",
    "#                           pretrained=False,      \n",
    "#                           num_classes=10)\n",
    "# model.to(DEVICE)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "\n",
    "# def run_epoch(loader, train: bool):\n",
    "#     model.train(train)\n",
    "#     acc = MulticlassAccuracy(num_classes=10).to(DEVICE)\n",
    "#     loss_sum = 0\n",
    "#     for images, labels in loader:\n",
    "#         images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "#         if train:\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#         with torch.set_grad_enabled(train):\n",
    "#             logits = model(images)\n",
    "#             loss   = criterion(logits, labels)\n",
    "#             if train:\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "\n",
    "#         loss_sum += loss.item() * images.size(0)\n",
    "#         acc.update(logits.softmax(1), labels)\n",
    "\n",
    "#     return loss_sum / len(loader.dataset), acc.compute().item()\n",
    "\n",
    "# history = {\"tr_loss\":[], \"tr_acc\":[], \"val_loss\":[], \"val_acc\":[]}\n",
    "\n",
    "# for epoch in range(1, NUM_EPOCHS+1):\n",
    "#     tr_loss, tr_acc = run_epoch(train_loader, train=True)\n",
    "#     vl_loss, vl_acc = run_epoch(val_loader,   train=False)\n",
    "\n",
    "#     history[\"tr_loss\"].append(tr_loss)\n",
    "#     history[\"tr_acc\"].append(tr_acc)\n",
    "#     history[\"val_loss\"].append(vl_loss)\n",
    "#     history[\"val_acc\"].append(vl_acc)\n",
    "\n",
    "#     print(f\"[{epoch:02d}/{NUM_EPOCHS}] \"\n",
    "#           f\"train acc {tr_acc:.3%} | val acc {vl_acc:.3%}\")\n",
    "\n",
    "# torch.save(model.state_dict(), \"deit_tiny_cifar10_scratch_2.pth\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64e0a638-b1ba-4cf1-b9ec-99ea775f1e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_268611/1389627459.py:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"deit_tiny_cifar10_scratch_2.pth\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tiny‑scratch]  Top‑1: 78.07%,  Top‑5: 97.32%\n",
      "[Small‑distilled]  Top‑1: 5.04%,  Top‑5: 43.47%\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import torch\n",
    "import torchvision.datasets as ds\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "import timm\n",
    "\n",
    "# --- 0. Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "IMG_SIZE = 224\n",
    "batch_size = 128\n",
    "root = pathlib.Path.home() / \"data-cifar10\"\n",
    "\n",
    "# transforms + loader\n",
    "test_tfms = T.Compose([\n",
    "    T.Resize(IMG_SIZE),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=(0.5,)*3, std=(0.5,)*3),\n",
    "])\n",
    "test_set    = ds.CIFAR10(root, train=False, download=True, transform=test_tfms)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# evaluation helper\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    top1_correct = 0\n",
    "    top5_correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            logits = model(imgs)\n",
    "\n",
    "            # Top-1\n",
    "            _, p1 = logits.topk(1, dim=1)\n",
    "            top1_correct += (p1.squeeze(1) == labels).sum().item()\n",
    "\n",
    "            # Top-5\n",
    "            _, p5 = logits.topk(5, dim=1)\n",
    "            top5_correct += sum(int(labels[i] in p5[i]) for i in range(labels.size(0)))\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return 100 * top1_correct/total, 100 * top5_correct/total\n",
    "\n",
    "# --- 1. Tiny DeiT (scratch) ---\n",
    "# a) rebuild arch\n",
    "model_tiny = timm.create_model(\"deit_tiny_patch16_224\", pretrained=False, num_classes=10)\n",
    "# b) load your raw state_dict\n",
    "state_dict = torch.load(\"deit_tiny_cifar10_scratch_2.pth\", map_location=device)\n",
    "model_tiny.load_state_dict(state_dict)\n",
    "model_tiny.to(device)\n",
    "\n",
    "# c) eval\n",
    "t1, t5 = evaluate(model_tiny, test_loader)\n",
    "print(f\"[Tiny‑scratch]  Top‑1: {t1:.2f}%,  Top‑5: {t5:.2f}%\")\n",
    "\n",
    "# --- 2. Small DeiT Distilled (ImageNet pretrained) ---\n",
    "model_small = timm.create_model(\"deit_base_distilled_patch16_224\", pretrained=True, num_classes=10)\n",
    "model_small.to(device)\n",
    "\n",
    "t1s, t5s = evaluate(model_small, test_loader)\n",
    "print(f\"[Small‑distilled]  Top‑1: {t1s:.2f}%,  Top‑5: {t5s:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ce7d01c-6eed-494b-b306-42ba4a54c520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 0 done\n",
      "Epoch 1 done\n",
      "Epoch 2 done\n",
      "Epoch 3 done\n",
      "Epoch 4 done\n",
      "After fine‑tuning → 93.07 99.84\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BATCH      = 128           \n",
    "IMG_SIZE   = 224             \n",
    "WORKERS    = os.cpu_count() \n",
    "train_tfms = T.Compose([\n",
    "    T.Resize(IMG_SIZE),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=(0.5,)*3, std=(0.5,)*3),\n",
    "])\n",
    "\n",
    "test_tfms  = T.Compose([\n",
    "    T.Resize(IMG_SIZE),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=(0.5,)*3, std=(0.5,)*3),\n",
    "])\n",
    "root = pathlib.Path.home() / \"data-cifar10\"\n",
    "full_train = ds.CIFAR10(root, train=True,  download=True, transform=train_tfms)\n",
    "test_set   = ds.CIFAR10(root, train=False, download=True, transform=test_tfms)\n",
    "\n",
    "train_set, val_set = random_split(full_train, [45_000, 5_000],\n",
    "                                  generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(train_set, BATCH, shuffle=True,  num_workers=WORKERS)\n",
    "val_loader   = DataLoader(val_set,   BATCH, shuffle=False, num_workers=WORKERS)\n",
    "test  = DataLoader(test_set,  BATCH, shuffle=False, num_workers=WORKERS)\n",
    "model = timm.create_model(\"deit_small_distilled_patch16_224\",\n",
    "                          pretrained=True,\n",
    "                          num_classes=10)\n",
    "model.to(device)\n",
    "\n",
    "# freeze everything except the head (optional)\n",
    "# for name, p in model.named_parameters():\n",
    "#     if \"head\" not in name:\n",
    "#         p.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad,\n",
    "                                    model.parameters()),\n",
    "                              lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# simple fine‑tune loop\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch\", epoch, \"done\")\n",
    "\n",
    "# now eval…\n",
    "t1, t5 = evaluate(model, test_loader)\n",
    "print(\"After fine‑tuning →\", t1, t5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1397d64-4961-474d-b339-3e4164066379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BATCH      = 128           \n",
    "IMG_SIZE   = 224             \n",
    "WORKERS    = os.cpu_count() \n",
    "train_tfms = T.Compose([\n",
    "    T.Resize(IMG_SIZE),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=(0.5,)*3, std=(0.5,)*3),\n",
    "])\n",
    "\n",
    "test_tfms  = T.Compose([\n",
    "    T.Resize(IMG_SIZE),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=(0.5,)*3, std=(0.5,)*3),\n",
    "])\n",
    "root = pathlib.Path.home() / \"data-cifar10\"\n",
    "full_train = ds.CIFAR10(root, train=True,  download=True, transform=train_tfms)\n",
    "test_set   = ds.CIFAR10(root, train=False, download=True, transform=test_tfms)\n",
    "\n",
    "train_set, val_set = random_split(full_train, [45_000, 5_000],\n",
    "                                  generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(train_set, BATCH, shuffle=True,  num_workers=WORKERS)\n",
    "val_loader   = DataLoader(val_set,   BATCH, shuffle=False, num_workers=WORKERS)\n",
    "test  = DataLoader(test_set,  BATCH, shuffle=False, num_workers=WORKERS)\n",
    "model = timm.create_model(\"deit_small_distilled_patch16_224\",\n",
    "                          pretrained=True,\n",
    "                          num_classes=10)\n",
    "model.to(device)\n",
    "\n",
    "# freeze everything except the head (optional)\n",
    "# for name, p in model.named_parameters():\n",
    "#     if \"head\" not in name:\n",
    "#         p.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad,\n",
    "                                    model.parameters()),\n",
    "                              lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# simple fine‑tune loop\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch\", epoch, \"done\")\n",
    "\n",
    "# now eval…\n",
    "t1, t5 = evaluate(model, test_loader)\n",
    "print(\"After fine‑tuning →\", t1, t5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a542ce-c00b-4845-95d6-a8dc4ba5b23f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
